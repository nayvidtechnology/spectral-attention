{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e466bee3",
   "metadata": {},
   "source": [
    "## Load a tiny text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5360841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1013"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "text = \"\\n\".join(ds[\"train\"][\"text\"])\n",
    "# quick tokenizer to small vocab (byte-level or simple char-level):\n",
    "vocab = sorted(set(list(text)))\n",
    "stoi = {ch:i for i,ch in enumerate(vocab)}\n",
    "data_ids = torch.tensor([stoi.get(ch, 0) for ch in text], dtype=torch.long)\n",
    "vocab_size = len(vocab); vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24bfb26",
   "metadata": {},
   "source": [
    "## Train both models briefly, log ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3707dbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SpectralEncoder' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspectral_attention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_eval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_model, train_tiny_lm\n\u001b[1;32m----> 2\u001b[0m logs_spec \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tiny_lm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmake_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspectral\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m logs_van  \u001b[38;5;241m=\u001b[39m train_tiny_lm(make_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvanilla\u001b[39m\u001b[38;5;124m\"\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), data_ids, vocab_size, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "File \u001b[1;32mC:\\workspace\\AI\\spectral-attention\\src\\spectral_attention\\train_eval.py:41\u001b[0m, in \u001b[0;36mtrain_tiny_lm\u001b[1;34m(model, data_ids, vocab_size, steps, lr, bsz, T, log_path, device)\u001b[0m\n\u001b[0;32m     39\u001b[0m device \u001b[38;5;241m=\u001b[39m device \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 41\u001b[0m head \u001b[38;5;241m=\u001b[39m lm_head(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mff[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39min_features, vocab_size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     42\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlist\u001b[39m(head\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     43\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[1;32mc:\\Users\\dpanc\\miniconda3\\envs\\.venvmultigpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1926\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1927\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1930\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SpectralEncoder' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "from spectral_attention.train_eval import make_model, train_tiny_lm\n",
    "logs_spec = train_tiny_lm(make_model(\"spectral\", depth=2), data_ids, vocab_size, steps=200, T=512)\n",
    "logs_van  = train_tiny_lm(make_model(\"vanilla\", depth=2), data_ids, vocab_size, steps=200, T=512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d49632",
   "metadata": {},
   "source": [
    "## Plot ppl curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08ce8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([l[\"step\"] for l in logs_spec], [l[\"ppl\"] for l in logs_spec], label=\"spectral\")\n",
    "plt.plot([l[\"step\"] for l in logs_van],  [l[\"ppl\"] for l in logs_van],  label=\"vanilla\")\n",
    "plt.legend(); plt.ylabel(\"perplexity\"); plt.xlabel(\"step\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32233068",
   "metadata": {},
   "source": [
    "## Long-range toy (copy task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee4e97",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m s\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m20\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m: logs\u001b[38;5;241m.\u001b[39mappend((s, loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logs\n\u001b[1;32m---> 23\u001b[0m logs_s \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspectral\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m logs_v \u001b[38;5;241m=\u001b[39m train_copy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvanilla\u001b[39m\u001b[38;5;124m\"\u001b[39m,  T\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m)\n\u001b[0;32m     25\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mlogs_s), label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectral\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m, in \u001b[0;36mtrain_copy\u001b[1;34m(kind, steps, T)\u001b[0m\n\u001b[0;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m E, y, V, D \u001b[38;5;241m=\u001b[39m make_copy_batch(T\u001b[38;5;241m=\u001b[39mT, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmake_model\u001b[49m(kind, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, d_model\u001b[38;5;241m=\u001b[39mD, n_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(D, V, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlist\u001b[39m(head\u001b[38;5;241m.\u001b[39mparameters()), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "def make_copy_batch(bsz=32, T=1024, vocab_size=32, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "    x = torch.randint(1, vocab_size, (bsz, T), device=device)\n",
    "    y = x.clone()\n",
    "    D = 512\n",
    "    E = torch.nn.functional.one_hot(x, num_classes=vocab_size).float() @ torch.randn(vocab_size, D, device=device)*0.02\n",
    "    return E, y, vocab_size, D\n",
    "def train_copy(kind, steps=300, T=2048):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    E, y, V, D = make_copy_batch(T=T, device=device)\n",
    "    model = make_model(kind, depth=2, d_model=D, n_heads=8).to(device)\n",
    "    head = nn.Linear(D, V, bias=False).to(device)\n",
    "    opt = torch.optim.AdamW(list(model.parameters())+list(head.parameters()), lr=3e-4)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    logs=[]\n",
    "    for s in range(1, steps+1):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        h = model(E)\n",
    "        loss = loss_fn(head(h).reshape(-1, V), y.reshape(-1))\n",
    "        loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()\n",
    "        if s%20==0: logs.append((s, loss.item()))\n",
    "    return logs\n",
    "logs_s = train_copy(\"spectral\", T=4096)\n",
    "logs_v = train_copy(\"vanilla\",  T=4096)\n",
    "plt.plot(*zip(*logs_s), label=\"spectral\")\n",
    "plt.plot(*zip(*logs_v), label=\"vanilla\")\n",
    "plt.legend(); plt.ylabel(\"loss\"); plt.xlabel(\"step\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvmultigpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
