{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67e26e75",
   "metadata": {},
   "source": [
    "# Notebook: Scaling Law Analysis of Spectral vs. Vanilla Attention\n",
    "\n",
    "**Objective:** To empirically validate the computational complexity and performance of Spectral Attention (`O(n log n)`) against standard Self-Attention (`O(n^2)`).\n",
    "\n",
    "We will perform a micro-benchmark by running both models across a range of sequence lengths and measure three key metrics:\n",
    "1.  **Throughput** (tokens/second)\n",
    "2.  **Latency** (ms/iteration)\n",
    "3.  **Peak GPU Memory** (MB)\n",
    "\n",
    "The expectation is to observe a significant performance advantage for Spectral Attention, especially at longer sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaad9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Configuration is ready.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Cell 2: Setup and Configuration\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import importlib\n",
    "import platform\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Resolve script path robustly ---\n",
    "if os.path.exists(\"scripts/bench_spectral_attention.py\"):\n",
    "    BENCH_SCRIPT = os.path.abspath(\"scripts/bench_spectral_attention.py\")\n",
    "else:\n",
    "    # If running from notebooks/ directory, go up one level\n",
    "    maybe = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"scripts\", \"bench_spectral_attention.py\"))\n",
    "    if os.path.exists(maybe):\n",
    "        BENCH_SCRIPT = maybe\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not locate scripts/bench_spectral_attention.py from current working directory\")\n",
    "\n",
    "# --- Experiment Configuration ---\n",
    "\n",
    "# Path to the log file where results will be stored\n",
    "# The provided README mentions logs are stored in 'experiments/runs/**'.\n",
    "# We will create a specific directory for this analysis.\n",
    "LOG_DIR = \"experiments/runs/scaling_analysis\"\n",
    "LOG_FILE = os.path.join(LOG_DIR, \"metrics.jsonl\")\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# Clear previous log file if it exists\n",
    "if os.path.exists(LOG_FILE):\n",
    "    os.remove(LOG_FILE)\n",
    "    print(f\"Removed old log file: {LOG_FILE}\")\n",
    "\n",
    "# --- Model & Benchmark Parameters ---\n",
    "\n",
    "# Sequence lengths to test. We'll go up to a point where vanilla attention likely fails.\n",
    "SEQ_LENGTHS = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "VANILLA_MAX_SEQ = 4096 # The sequence length at which vanilla attention often runs out of memory on consumer GPUs\n",
    "\n",
    "# Fixed model parameters for a fair comparison\n",
    "# These parameters are based on the examples in the README.\n",
    "DMODEL = 512\n",
    "HEADS = 8\n",
    "DEPTH = 6\n",
    "BATCH = 4\n",
    "DEVICE = \"gpu\"  # Force GPU if available\n",
    "USE_COMPILE = False  # Disable torch.compile to avoid CUDA/Triton conflicts\n",
    "\n",
    "def get_compile_flag():\n",
    "    \"\"\"Return ['--compile'] only when explicitly enabled and environment supports it.\"\"\"\n",
    "    if not USE_COMPILE:\n",
    "        return []\n",
    "    # Require CUDA + Triton + non-Windows (inductor+triton works best on Linux)\n",
    "    cuda_ok = False\n",
    "    try:\n",
    "        import torch\n",
    "        cuda_ok = torch.cuda.is_available()\n",
    "    except Exception:\n",
    "        cuda_ok = False\n",
    "    triton_ok = importlib.util.find_spec(\"triton\") is not None\n",
    "    on_windows = platform.system().lower().startswith(\"win\")\n",
    "    if cuda_ok and triton_ok and not on_windows:\n",
    "        return [\"--compile\"]\n",
    "    print(\"[warn] --compile disabled at runtime: CUDA/Triton not available or unsupported OS\")\n",
    "    return []\n",
    "\n",
    "def effective_batch(seq:int, kind:str) -> int:\n",
    "    \"\"\"Use smaller batch for vanilla at long seq to avoid OOM on consumer GPUs.\"\"\"\n",
    "    if kind == \"vanilla\" and seq >= VANILLA_MAX_SEQ:\n",
    "        return max(1, BATCH // 4)  # drop batch when seq is very large\n",
    "    return BATCH\n",
    "\n",
    "print(\"Setup complete. Configuration is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca7bf1",
   "metadata": {},
   "source": [
    "### Running the Benchmark\n",
    "\n",
    "Now, we will execute the benchmark script (`scripts/bench_spectral_attention.py`) using the parameters defined above. We will loop through each sequence length and run the benchmark for both the `spectral` and `vanilla` models.\n",
    "\n",
    "The output of each run will be appended to our log file (`experiments/runs/scaling_analysis/metrics.jsonl`). We will stop benchmarking the `vanilla` model after it reaches its maximum defined sequence length to avoid out-of-memory errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90858b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark execution...\n",
      "-> Running SPECTRAL with Sequence Length: 512\n",
      "Device=gpu  (tensor on cuda)  B=4  T=512  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 219,589   ms/iter: 9.33   peakMB: 102.5\n",
      "\n",
      "-> Running VANILLA with Sequence Length: 512\n",
      "Device=gpu  (tensor on cuda)  B=4  T=512  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 219,589   ms/iter: 9.33   peakMB: 102.5\n",
      "\n",
      "-> Running VANILLA with Sequence Length: 512\n",
      "Device=gpu  (tensor on cuda)  B=4  T=512  d_model=512  heads=8  depth=6  kind=vanilla\n",
      "     vanilla  tokens/s: 144,775   ms/iter: 14.15   peakMB: 130.3\n",
      "\n",
      "-> Running SPECTRAL with Sequence Length: 1024\n",
      "Device=gpu  (tensor on cuda)  B=4  T=512  d_model=512  heads=8  depth=6  kind=vanilla\n",
      "     vanilla  tokens/s: 144,775   ms/iter: 14.15   peakMB: 130.3\n",
      "\n",
      "-> Running SPECTRAL with Sequence Length: 1024\n",
      "Device=gpu  (tensor on cuda)  B=4  T=1024  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 225,204   ms/iter: 18.19   peakMB: 168.7\n",
      "\n",
      "-> Running VANILLA with Sequence Length: 1024\n",
      "Device=gpu  (tensor on cuda)  B=4  T=1024  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 225,204   ms/iter: 18.19   peakMB: 168.7\n",
      "\n",
      "-> Running VANILLA with Sequence Length: 1024\n",
      "Device=gpu  (tensor on cuda)  B=4  T=1024  d_model=512  heads=8  depth=6  kind=vanilla\n",
      "     vanilla  tokens/s: 108,624   ms/iter: 37.71   peakMB: 178.3\n",
      "\n",
      "-> Running SPECTRAL with Sequence Length: 2048\n",
      "Device=gpu  (tensor on cuda)  B=4  T=1024  d_model=512  heads=8  depth=6  kind=vanilla\n",
      "     vanilla  tokens/s: 108,624   ms/iter: 37.71   peakMB: 178.3\n",
      "\n",
      "-> Running SPECTRAL with Sequence Length: 2048\n",
      "Device=gpu  (tensor on cuda)  B=4  T=2048  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 169,722   ms/iter: 48.27   peakMB: 305.1\n",
      "\n",
      "-> Running VANILLA with Sequence Length: 2048\n",
      "Device=gpu  (tensor on cuda)  B=4  T=2048  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 169,722   ms/iter: 48.27   peakMB: 305.1\n",
      "\n",
      "-> Running VANILLA with Sequence Length: 2048\n",
      "Device=gpu  (tensor on cuda)  B=4  T=2048  d_model=512  heads=8  depth=6  kind=vanilla\n",
      "     vanilla  tokens/s: 69,229   ms/iter: 118.33   peakMB: 274.2\n",
      "\n",
      "-> Running SPECTRAL with Sequence Length: 4096\n",
      "Device=gpu  (tensor on cuda)  B=4  T=2048  d_model=512  heads=8  depth=6  kind=vanilla\n",
      "     vanilla  tokens/s: 69,229   ms/iter: 118.33   peakMB: 274.2\n",
      "\n",
      "-> Running SPECTRAL with Sequence Length: 4096\n",
      "Device=gpu  (tensor on cuda)  B=4  T=4096  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 166,547   ms/iter: 98.37   peakMB: 577.9\n",
      "\n",
      "-> Running VANILLA with Sequence Length: 4096\n",
      "Device=gpu  (tensor on cuda)  B=4  T=4096  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 166,547   ms/iter: 98.37   peakMB: 577.9\n",
      "\n",
      "-> Running VANILLA with Sequence Length: 4096\n",
      "Device=gpu  (tensor on cuda)  B=1  T=4096  d_model=512  heads=8  depth=6  kind=vanilla\n",
      "     vanilla  tokens/s: 34,942   ms/iter: 117.22   peakMB: 178.3\n",
      "\n",
      "-> Running SPECTRAL with Sequence Length: 8192\n",
      "Device=gpu  (tensor on cuda)  B=1  T=4096  d_model=512  heads=8  depth=6  kind=vanilla\n",
      "     vanilla  tokens/s: 34,942   ms/iter: 117.22   peakMB: 178.3\n",
      "\n",
      "-> Running SPECTRAL with Sequence Length: 8192\n",
      "Device=gpu  (tensor on cuda)  B=4  T=8192  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 157,982   ms/iter: 207.42   peakMB: 1123.6\n",
      "\n",
      "-> Skipping VANILLA for Sequence Length 8192 (>= 4096)\n",
      "-> Running SPECTRAL with Sequence Length: 16384\n",
      "Device=gpu  (tensor on cuda)  B=4  T=8192  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 157,982   ms/iter: 207.42   peakMB: 1123.6\n",
      "\n",
      "-> Skipping VANILLA for Sequence Length 8192 (>= 4096)\n",
      "-> Running SPECTRAL with Sequence Length: 16384\n",
      "Device=gpu  (tensor on cuda)  B=4  T=16384  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 131,140   ms/iter: 499.74   peakMB: 2214.9\n",
      "\n",
      "-> Skipping VANILLA for Sequence Length 16384 (>= 4096)\n",
      "\n",
      "Benchmark execution complete!\n",
      "Device=gpu  (tensor on cuda)  B=4  T=16384  d_model=512  heads=8  depth=6  kind=spectral\n",
      "spectral_dct  tokens/s: 131,140   ms/iter: 499.74   peakMB: 2214.9\n",
      "\n",
      "-> Skipping VANILLA for Sequence Length 16384 (>= 4096)\n",
      "\n",
      "Benchmark execution complete!\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Cell 4: Execute Benchmark Script\n",
    "#\n",
    "import sys\n",
    "print(\"Starting benchmark execution...\")\n",
    "\n",
    "\n",
    "def run_cmd(cmd: list[str]):\n",
    "    proc = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    if proc.returncode != 0:\n",
    "        print(\"[stderr]\\n\" + (proc.stderr or \"\"))\n",
    "        print(\"[stdout]\\n\" + (proc.stdout or \"\"))\n",
    "        raise RuntimeError(f\"Command failed ({proc.returncode}): {' '.join(cmd)}\")\n",
    "    if proc.stdout:\n",
    "        print(proc.stdout)\n",
    "\n",
    "for seq in SEQ_LENGTHS:\n",
    "    # --- Run Spectral Attention Benchmark ---\n",
    "    print(f\"-> Running SPECTRAL with Sequence Length: {seq}\")\n",
    "    spectral_command = [\n",
    "        sys.executable, BENCH_SCRIPT,\n",
    "        \"--kind\", \"spectral\",\n",
    "        \"--seq\", str(seq),\n",
    "        \"--dmodel\", str(DMODEL),\n",
    "        \"--heads\", str(HEADS),\n",
    "        \"--depth\", str(DEPTH),\n",
    "        \"--batch\", str(effective_batch(seq, \"spectral\")),\n",
    "        \"--device\", DEVICE,\n",
    "        \"--logdir\", LOG_DIR,  # Directing output to our specific log directory\n",
    "        *get_compile_flag(),\n",
    "    ]\n",
    "    run_cmd(spectral_command)\n",
    "\n",
    "    # --- Run Vanilla Attention Benchmark (with sequence length limit) ---\n",
    "    if seq <= VANILLA_MAX_SEQ:\n",
    "        print(f\"-> Running VANILLA with Sequence Length: {seq}\")\n",
    "        vanilla_command = [\n",
    "            sys.executable, BENCH_SCRIPT,\n",
    "            \"--kind\", \"vanilla\",\n",
    "            \"--seq\", str(seq),\n",
    "            \"--dmodel\", str(DMODEL),\n",
    "            \"--heads\", str(HEADS),\n",
    "            \"--depth\", str(DEPTH),\n",
    "            \"--batch\", str(effective_batch(seq, \"vanilla\")),\n",
    "            \"--device\", DEVICE,\n",
    "            \"--logdir\", LOG_DIR,\n",
    "            *get_compile_flag(),\n",
    "        ]\n",
    "        run_cmd(vanilla_command)\n",
    "    else:\n",
    "        print(f\"-> Skipping VANILLA for Sequence Length {seq} (>= {VANILLA_MAX_SEQ})\")\n",
    "\n",
    "print(\"\\nBenchmark execution complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvmultigpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
