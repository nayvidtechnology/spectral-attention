{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Attention Layer Demo\n",
    "\n",
    "This notebook demonstrates the new HybridAttentionLayer that allows mixing different attention types (standard, spectral, holonomy) within a single transformer layer.\n",
    "\n",
    "## Key Features\n",
    "- **Configurable head types**: Specify which attention mechanism each head should use\n",
    "- **Mixed attention**: Combine standard, spectral, and holonomy attention in one layer\n",
    "- **Flexible configuration**: Different numbers of each head type\n",
    "- **Performance analysis**: Compare different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the source directory to Python path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "from spectral_attention.layers.hybrid_layer import HybridAttentionLayer\n",
    "from spectral_attention import SpectralAttention, HolonomyAttention\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage\n",
    "\n",
    "Let's start with the acceptance criteria: a layer with [4 standard, 2 spectral, 2 holonomy] heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration matching the acceptance criteria\n",
    "d_model = 512\n",
    "head_types = [\"standard\"] * 4 + [\"spectral\"] * 2 + [\"holonomy\"] * 2\n",
    "\n",
    "print(f\"Head configuration: {head_types}\")\n",
    "print(f\"Total heads: {len(head_types)}\")\n",
    "print(f\"Model dimension: {d_model}\")\n",
    "print(f\"Dimension per head: {d_model // len(head_types)}\")\n",
    "\n",
    "# Create the hybrid layer\n",
    "hybrid_layer = HybridAttentionLayer(\n",
    "    d_model=d_model,\n",
    "    head_types=head_types,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"\\nLayer created successfully!\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in hybrid_layer.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input\n",
    "batch_size, seq_len = 4, 128\n",
    "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = hybrid_layer(x)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output statistics:\")\n",
    "print(f\"  Mean: {output.mean().item():.6f}\")\n",
    "print(f\"  Std: {output.std().item():.6f}\")\n",
    "print(f\"  Min: {output.min().item():.6f}\")\n",
    "print(f\"  Max: {output.max().item():.6f}\")\n",
    "\n",
    "# Check for NaN or Inf values\n",
    "print(f\"  Contains NaN: {torch.isnan(output).any().item()}\")\n",
    "print(f\"  Contains Inf: {torch.isinf(output).any().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's compare different attention configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_layer(layer, x, num_runs=10):\n",
    "    \"\"\"Benchmark a layer's forward pass.\"\"\"\n",
    "    layer.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(3):\n",
    "            _ = layer(x)\n",
    "    \n",
    "    # Benchmark\n",
    "    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_runs):\n",
    "            _ = layer(x)\n",
    "    \n",
    "    torch.cuda.synchronize() if device == \"cuda\" else None\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return (end_time - start_time) / num_runs\n",
    "\n",
    "# Test different configurations\n",
    "configurations = {\n",
    "    \"All Standard (8 heads)\": [\"standard\"] * 8,\n",
    "    \"All Spectral (8 heads)\": [\"spectral\"] * 8,\n",
    "    \"All Holonomy (8 heads)\": [\"holonomy\"] * 8,\n",
    "    \"Mixed (4+2+2)\": [\"standard\"] * 4 + [\"spectral\"] * 2 + [\"holonomy\"] * 2,\n",
    "    \"Balanced (3+3+2)\": [\"standard\"] * 3 + [\"spectral\"] * 3 + [\"holonomy\"] * 2,\n",
    "}\n",
    "\n",
    "# Small input for benchmarking\n",
    "bench_x = torch.randn(2, 64, d_model, device=device)\n",
    "\n",
    "results = {}\n",
    "for name, head_config in configurations.items():\n",
    "    layer = HybridAttentionLayer(\n",
    "        d_model=d_model,\n",
    "        head_types=head_config,\n",
    "        dropout=0.0  # Disable dropout for benchmarking\n",
    "    ).to(device)\n",
    "    \n",
    "    avg_time = benchmark_layer(layer, bench_x)\n",
    "    results[name] = avg_time * 1000  # Convert to milliseconds\n",
    "    print(f\"{name}: {avg_time * 1000:.2f} ms\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "names = list(results.keys())\n",
    "times = list(results.values())\n",
    "\n",
    "bars = plt.bar(names, times, color=['skyblue', 'lightgreen', 'lightcoral', 'gold', 'plum'])\n",
    "plt.ylabel('Average Time (ms)')\n",
    "plt.title('Performance Comparison of Different Hybrid Attention Configurations')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, time_val in zip(bars, times):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "             f'{time_val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Pattern Analysis\n",
    "\n",
    "Let's analyze how different head types affect the attention patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_attention_outputs(layer, x, head_types):\n",
    "    \"\"\"Analyze the outputs from different attention heads.\"\"\"\n",
    "    layer.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get intermediate outputs by modifying the forward pass temporarily\n",
    "        B, T, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        qkv = layer.W_qkv(x)  # [B, T, 3*d_model]\n",
    "        qkv = qkv.view(B, T, 3, layer.n_heads, layer.d_head)  # [B, T, 3, H, D]\n",
    "        q, k, v = qkv.permute(2, 0, 3, 1, 4)  # [3, B, H, T, D]\n",
    "        \n",
    "        # Process heads by type and collect outputs\n",
    "        head_outputs = {}\n",
    "        head_idx = 0\n",
    "        \n",
    "        for head_type in [\"standard\", \"spectral\", \"holonomy\"]:\n",
    "            if head_type not in layer.head_type_counts:\n",
    "                continue\n",
    "                \n",
    "            count = layer.head_type_counts[head_type]\n",
    "            \n",
    "            # Extract heads for this type\n",
    "            q_heads = q[:, head_idx:head_idx + count]  # [B, count, T, D]\n",
    "            k_heads = k[:, head_idx:head_idx + count]  # [B, count, T, D]\n",
    "            v_heads = v[:, head_idx:head_idx + count]  # [B, count, T, D]\n",
    "            \n",
    "            # Apply attention\n",
    "            attn_module = layer.attention_modules[head_type]\n",
    "            out_heads = attn_module(q_heads, k_heads, v_heads)  # [B, count, T, D]\n",
    "            \n",
    "            head_outputs[head_type] = out_heads\n",
    "            head_idx += count\n",
    "    \n",
    "    return head_outputs\n",
    "\n",
    "# Analyze the mixed configuration\n",
    "test_x = torch.randn(1, 32, d_model, device=device)\n",
    "head_outputs = analyze_attention_outputs(hybrid_layer, test_x, head_types)\n",
    "\n",
    "# Plot statistics for each head type\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle('Attention Head Output Analysis', fontsize=16)\n",
    "\n",
    "colors = {'standard': 'skyblue', 'spectral': 'lightgreen', 'holonomy': 'lightcoral'}\n",
    "\n",
    "# Mean values\n",
    "ax1 = axes[0, 0]\n",
    "means = [head_outputs[ht].mean().item() for ht in head_outputs.keys()]\n",
    "bars1 = ax1.bar(head_outputs.keys(), means, color=[colors[ht] for ht in head_outputs.keys()])\n",
    "ax1.set_title('Mean Output Values')\n",
    "ax1.set_ylabel('Mean')\n",
    "\n",
    "# Standard deviation\n",
    "ax2 = axes[0, 1]\n",
    "stds = [head_outputs[ht].std().item() for ht in head_outputs.keys()]\n",
    "bars2 = ax2.bar(head_outputs.keys(), stds, color=[colors[ht] for ht in head_outputs.keys()])\n",
    "ax2.set_title('Output Standard Deviation')\n",
    "ax2.set_ylabel('Std Dev')\n",
    "\n",
    "# Max values\n",
    "ax3 = axes[1, 0]\n",
    "maxs = [head_outputs[ht].max().item() for ht in head_outputs.keys()]\n",
    "bars3 = ax3.bar(head_outputs.keys(), maxs, color=[colors[ht] for ht in head_outputs.keys()])\n",
    "ax3.set_title('Max Output Values')\n",
    "ax3.set_ylabel('Max')\n",
    "\n",
    "# Min values\n",
    "ax4 = axes[1, 1]\n",
    "mins = [head_outputs[ht].min().item() for ht in head_outputs.keys()]\n",
    "bars4 = ax4.bar(head_outputs.keys(), mins, color=[colors[ht] for ht in head_outputs.keys()])\n",
    "ax4.set_title('Min Output Values')\n",
    "ax4.set_ylabel('Min')\n",
    "\n",
    "# Add value labels\n",
    "for bars, values in [(bars1, means), (bars2, stds), (bars3, maxs), (bars4, mins)]:\n",
    "    for bar, val in zip(bars, values):\n",
    "        bar.axes.text(bar.get_x() + bar.get_width()/2, bar.get_height() + bar.get_height()*0.01,\n",
    "                     f'{val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHead Type Counts:\")\n",
    "for head_type, output in head_outputs.items():\n",
    "    print(f\"{head_type}: {output.shape[1]} heads, output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Flow Analysis\n",
    "\n",
    "Let's check how gradients flow through different head types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new layer for gradient analysis\n",
    "grad_layer = HybridAttentionLayer(\n",
    "    d_model=d_model,\n",
    "    head_types=head_types,\n",
    "    dropout=0.0\n",
    ").to(device)\n",
    "\n",
    "# Create input that requires gradients\n",
    "grad_x = torch.randn(2, 16, d_model, device=device, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "output = grad_layer(grad_x)\n",
    "loss = output.sum()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(\"Gradient Analysis:\")\n",
    "print(f\"Input gradient norm: {grad_x.grad.norm().item():.6f}\")\n",
    "\n",
    "# Analyze gradients in different components\n",
    "gradient_norms = {}\n",
    "\n",
    "for name, param in grad_layer.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        gradient_norms[name] = param.grad.norm().item()\n",
    "\n",
    "# Group gradients by attention type\n",
    "attention_grads = {}\n",
    "for name, norm in gradient_norms.items():\n",
    "    if 'attention_modules' in name:\n",
    "        parts = name.split('.')\n",
    "        attn_type = parts[1]  # Extract attention type\n",
    "        if attn_type not in attention_grads:\n",
    "            attention_grads[attn_type] = []\n",
    "        attention_grads[attn_type].append(norm)\n",
    "    else:\n",
    "        print(f\"{name}: {norm:.6f}\")\n",
    "\n",
    "print(\"\\nAttention Module Gradients:\")\n",
    "avg_grads = {}\n",
    "for attn_type, grads in attention_grads.items():\n",
    "    avg_grad = np.mean(grads)\n",
    "    avg_grads[attn_type] = avg_grad\n",
    "    print(f\"{attn_type}: avg {avg_grad:.6f}, max {max(grads):.6f}, min {min(grads):.6f}\")\n",
    "\n",
    "# Plot gradient magnitudes\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(avg_grads.keys(), avg_grads.values(), \n",
    "        color=[colors[k] for k in avg_grads.keys()])\n",
    "plt.title('Average Gradient Magnitudes by Attention Type')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.yscale('log')\n",
    "\n",
    "for i, (k, v) in enumerate(avg_grads.items()):\n",
    "    plt.text(i, v * 1.1, f'{v:.2e}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Examples\n",
    "\n",
    "Here are some example configurations for different use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different configuration examples\n",
    "example_configs = {\n",
    "    \"Balanced Multi-modal\": {\n",
    "        \"heads\": [\"standard\", \"spectral\", \"holonomy\"] * 2 + [\"standard\", \"spectral\"],\n",
    "        \"description\": \"Equal representation of all attention types for diverse patterns\"\n",
    "    },\n",
    "    \"Spectral-heavy\": {\n",
    "        \"heads\": [\"spectral\"] * 6 + [\"standard\", \"holonomy\"],\n",
    "        \"description\": \"Emphasizes frequency-domain processing for long sequences\"\n",
    "    },\n",
    "    \"Standard-dominant\": {\n",
    "        \"heads\": [\"standard\"] * 6 + [\"spectral\", \"holonomy\"],\n",
    "        \"description\": \"Mostly standard attention with some specialized heads\"\n",
    "    },\n",
    "    \"Alternating Pattern\": {\n",
    "        \"heads\": [\"standard\", \"spectral\", \"holonomy\", \"standard\", \n",
    "                  \"spectral\", \"holonomy\", \"standard\", \"standard\"],\n",
    "        \"description\": \"Alternating pattern for diverse information processing\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Example Hybrid Attention Configurations:\\n\")\n",
    "for name, config in example_configs.items():\n",
    "    heads = config[\"heads\"]\n",
    "    counts = {ht: heads.count(ht) for ht in set(heads)}\n",
    "    \n",
    "    print(f\"**{name}**\")\n",
    "    print(f\"Description: {config['description']}\")\n",
    "    print(f\"Configuration: {heads}\")\n",
    "    print(f\"Head counts: {counts}\")\n",
    "    print(f\"Total heads: {len(heads)}\")\n",
    "    \n",
    "    # Test the configuration\n",
    "    try:\n",
    "        test_layer = HybridAttentionLayer(d_model=d_model, head_types=heads)\n",
    "        test_input = torch.randn(1, 16, d_model)\n",
    "        with torch.no_grad():\n",
    "            test_output = test_layer(test_input)\n",
    "        print(f\"✓ Configuration works! Output shape: {test_output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Configuration failed: {e}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "The HybridAttentionLayer successfully implements the requirements:\n",
    "\n",
    "### ✅ Acceptance Criteria Met:\n",
    "1. **Configurable list of head types per layer** - ✓ Implemented with `head_types` parameter\n",
    "2. **Concatenate outputs across heads → project back** - ✓ Implemented with proper concatenation and output projection\n",
    "3. **Unit test: run forward with [4 standard, 2 spectral, 2 holonomy]** - ✓ Tested and working\n",
    "\n",
    "### Key Features:\n",
    "- **Flexible Configuration**: Support any combination of attention types\n",
    "- **Proper Interface**: Compatible with existing transformer architectures\n",
    "- **Gradient Flow**: Proper backpropagation through all head types\n",
    "- **Performance**: Reasonable computational overhead\n",
    "\n",
    "### Usage Patterns:\n",
    "```python\n",
    "# Basic usage\n",
    "head_types = [\"standard\"] * 4 + [\"spectral\"] * 2 + [\"holonomy\"] * 2\n",
    "layer = HybridAttentionLayer(d_model=512, head_types=head_types)\n",
    "output = layer(input_tensor)  # [batch, seq_len, d_model]\n",
    "```\n",
    "\n",
    "This implementation allows researchers and practitioners to experiment with different combinations of attention mechanisms within a single layer, potentially capturing different types of patterns and relationships in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}